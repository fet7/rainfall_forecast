{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74b342ab-cc56-49fc-8e4a-65cc2d8a8ce5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Invalid Month entries found: <DatetimeArray>\n",
      "['1993-01-01 00:00:00', '1993-02-01 00:00:00', '1993-03-01 00:00:00',\n",
      " '1993-04-01 00:00:00', '1993-05-01 00:00:00', '1993-06-01 00:00:00',\n",
      " '1993-07-01 00:00:00', '1993-08-01 00:00:00', '1993-09-01 00:00:00',\n",
      " '1993-10-01 00:00:00',\n",
      " ...\n",
      " '2022-05-01 00:00:00', '2022-06-01 00:00:00', '2022-07-01 00:00:00',\n",
      " '2022-08-01 00:00:00', '2022-09-01 00:00:00', '2022-10-01 00:00:00',\n",
      " '2022-11-01 00:00:00', '2022-12-01 00:00:00', '2023-01-01 00:00:00',\n",
      " '2023-02-01 00:00:00']\n",
      "Length: 360, dtype: datetime64[ns]\n",
      "ERROR:root:Error converting YEAR and Month to datetime: Some values in 'YEAR' and 'Month' could not be converted to datetime.\n",
      "ERROR:root:Unexpected error in handle_missing_values: YEAR or Month columns may still contain invalid data after cleaning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              NAME     GH_ID   GEOGR2   GEOGR1  ELEVATION Element  YEAR  \\\n",
      "0  Addis Ababa Obs  SHADDI21  9.01891  38.7475       2386  PRECIP  1993   \n",
      "1  Addis Ababa Obs  SHADDI21  9.01891  38.7475       2386  PRECIP  1993   \n",
      "2  Addis Ababa Obs  SHADDI21  9.01891  38.7475       2386  PRECIP  1993   \n",
      "3  Addis Ababa Obs  SHADDI21  9.01891  38.7475       2386  PRECIP  1993   \n",
      "4  Addis Ababa Obs  SHADDI21  9.01891  38.7475       2386  PRECIP  1993   \n",
      "\n",
      "   Month    1    2  ...   22    23   24    25   26    27   28   29   30   31  \n",
      "0      1  0.0  0.0  ...  0.0   0.0  7.8   0.1  0.0   0.0  0.0  0.0  0.0  2.5  \n",
      "1      2  0.0  0.0  ...  0.0   0.0  0.3   0.0  1.2   0.0  0.0  NaN  NaN  NaN  \n",
      "2      3  0.0  0.0  ...  0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
      "3      4  0.0  0.0  ...  3.4   0.0  0.0   5.5  0.0  42.8  0.0  0.6  0.0  NaN  \n",
      "4      5  0.0  0.0  ...  0.5  14.7  0.7  19.5  0.0   0.0  2.2  0.7  1.4  1.7  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "YEAR or Month columns may still contain invalid data after cleaning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 98\u001b[0m, in \u001b[0;36mhandle_missing_values\u001b[0;34m(df, drop_cells, drop_rows, threshold)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome values in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYEAR\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m could not be converted to datetime.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: Some values in 'YEAR' and 'Month' could not be converted to datetime.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 829\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m#     # Choose the file input method\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m#     # If you want to use a static file path, set it here\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m \n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m#Step 2: Handle missing values and redundant columns\u001b[39;00m\n\u001b[1;32m    828\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHandling missing values...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 829\u001b[0m df \u001b[38;5;241m=\u001b[39m handle_missing_values(df, drop_cells\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    831\u001b[0m     df \u001b[38;5;241m=\u001b[39m create_month_column(df)  \u001b[38;5;66;03m# Recreate if dropped\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 101\u001b[0m, in \u001b[0;36mhandle_missing_values\u001b[0;34m(df, drop_cells, drop_rows, threshold)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    100\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError converting YEAR and Month to datetime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYEAR or Month columns may still contain invalid data after cleaning.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Handle missing cells\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drop_cells:\n",
      "\u001b[0;31mValueError\u001b[0m: YEAR or Month columns may still contain invalid data after cleaning."
     ]
    }
   ],
   "source": [
    "#-----Import Necessary libraries and Packages-----------#\n",
    "import os\n",
    "import io\n",
    "import yaml\n",
    "import joblib\n",
    "import optuna\n",
    "import logging\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from arch import unitroot\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import FileUpload\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import FileLink\n",
    "import matplotlib.patches as mpatches\n",
    "from arch.unitroot import PhillipsPerron\n",
    "from plotly.subplots import make_subplots\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError, Huber\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, zivot_andrews\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, ConvLSTM2D, BatchNormalization\n",
    "\n",
    "#--------------------Predefined Workflow Functions-----------#\n",
    "def configure_logging(level=logging.INFO):\n",
    "    logging.basicConfig(level=level, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logging.info(\"Logging configured successfully.\")\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a specified file path.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Loaded Dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file path\n",
    "        if not isinstance(file_path, str) or not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Invalid file path: {file_path}\")\n",
    "\n",
    "        # Attempt to load the dataset\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')  # Use utf-8 by default\n",
    "        if df.empty:\n",
    "            raise ValueError(\"The dataset is empty.\")\n",
    "        \n",
    "        # Log successful load\n",
    "        logging.info(f\"Dataset successfully loaded from: {file_path}\")\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found at: {file_path}\")\n",
    "        raise\n",
    "    except pd.errors.ParserError:\n",
    "        logging.error(\"Failed to parse the dataset file. Ensure it is in CSV format.\")\n",
    "        raise\n",
    "    except UnicodeDecodeError:\n",
    "        logging.error(\"Encoding error: Ensure the dataset is UTF-8 encoded.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while loading the dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def handle_missing_values(df, drop_cells=False, drop_rows=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Handle missing and redundant values in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataset.\n",
    "    drop_cells (bool): Drop individual cells with missing values.\n",
    "    drop_rows (bool): Drop rows with missing values based on a threshold.\n",
    "    threshold (float): Fraction of missing values in a row to trigger row removal.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataset with missing values handled.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate and clean YEAR and Month columns\n",
    "        df = validate_year_month(df)\n",
    "\n",
    "        # Convert YEAR and Month to datetime\n",
    "        try:\n",
    "            df['Month'] = pd.to_datetime(df[['YEAR', 'Month']].assign(day=1), errors='coerce')\n",
    "            if df['Month'].isna().any():\n",
    "                raise ValueError(\"Some values in 'YEAR' and 'Month' could not be converted to datetime.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error converting YEAR and Month to datetime: {e}\")\n",
    "            raise ValueError(\"YEAR or Month columns may still contain invalid data after cleaning.\")\n",
    "\n",
    "        # Handle missing cells\n",
    "        if drop_cells:\n",
    "            missing_cells_count = df.isna().sum().sum()\n",
    "            logging.info(f\"Dropping columns with any missing values. Total missing cells: {missing_cells_count}\")\n",
    "            df = df.dropna(axis=1, how='any')\n",
    "\n",
    "        # Handle rows with excessive missing values\n",
    "        if drop_rows:\n",
    "            missing_rows_count = df.isna().any(axis=1).sum()\n",
    "            row_threshold = int(threshold * len(df.columns))\n",
    "            logging.info(f\"Dropping rows with more than {threshold*100}% missing values. Total rows affected: {missing_rows_count}\")\n",
    "            df = df.dropna(thresh=row_threshold)\n",
    "\n",
    "        logging.info(f\"Final dataset has {df.isna().sum().sum()} missing cells.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in handle_missing_values: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_features_and_metrics(\n",
    "    df,\n",
    "    daily_columns_prefix=\"\",\n",
    "    rolling_window=12,\n",
    "    validate_values=True,\n",
    "    negative_threshold=0,\n",
    "    log_level=logging.INFO\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract features and compute metrics like 'Monthly_Total' and 'Monthly_Average'.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input dataset.\n",
    "    - daily_columns_prefix (str): Prefix for daily rainfall columns (default is \"\").\n",
    "    - rolling_window (int): Rolling average window size.\n",
    "    - validate_values (bool): Validate rainfall values for anomalies.\n",
    "    - negative_threshold (float): Threshold for invalid values.\n",
    "    - log_level (int): Logging level.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated dataset with metrics.\n",
    "    - dict: Metadata about processing.\n",
    "    - list: List of identified daily rainfall columns.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=log_level)\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Starting feature extraction and metric computation.\")\n",
    "\n",
    "        # Identify daily columns dynamically\n",
    "        if daily_columns_prefix:\n",
    "            daily_columns = [col for col in df.columns if col.startswith(daily_columns_prefix)]\n",
    "        else:\n",
    "            # Fallback: Identify numeric columns that are not explicitly metadata\n",
    "            daily_columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['YEAR', 'Month']]\n",
    "\n",
    "        if not daily_columns:\n",
    "            raise KeyError(\"No daily rainfall columns found.\")\n",
    "\n",
    "        # Filter numeric daily columns\n",
    "        numeric_daily_columns = [col for col in daily_columns if pd.api.types.is_numeric_dtype(df[col])]\n",
    "        non_numeric_columns = set(daily_columns) - set(numeric_daily_columns)\n",
    "\n",
    "        if non_numeric_columns:\n",
    "            logging.warning(f\"Non-numeric data found in columns: {list(non_numeric_columns)}\")\n",
    "\n",
    "        # Optional value validation for numeric columns\n",
    "        if validate_values:\n",
    "            invalid_values = df[numeric_daily_columns][df[numeric_daily_columns] < negative_threshold]\n",
    "            if not invalid_values.empty:\n",
    "                logging.warning(f\"Detected invalid values below {negative_threshold}: {invalid_values.count().sum()} entries.\")\n",
    "\n",
    "        # Compute summary metrics for numeric columns\n",
    "        df['Monthly_Total'] = df[numeric_daily_columns].sum(axis=1, skipna=True)\n",
    "        df['Monthly_Average'] = df[numeric_daily_columns].mean(axis=1, skipna=True)\n",
    "        df['Rolling_Average'] = df['Monthly_Total'].rolling(window=rolling_window, min_periods=1).mean()\n",
    "\n",
    "        # Extract additional time-based features\n",
    "        df['Year'] = df['Month'].dt.year\n",
    "        df['Month_Name'] = df['Month'].dt.strftime('%B')\n",
    "\n",
    "        logging.info(\"Feature extraction and metric computation completed successfully.\")\n",
    "\n",
    "        # Return metadata for observability\n",
    "        metadata = {\n",
    "            \"daily_columns\": len(daily_columns),\n",
    "            \"numeric_columns\": len(numeric_daily_columns),\n",
    "            \"non_numeric_columns\": len(non_numeric_columns),\n",
    "            \"invalid_values\": invalid_values.count().sum() if validate_values else 0,\n",
    "        }\n",
    "        return df, metadata, daily_columns\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in extract_features_and_metrics: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in extract_features_and_metrics: {e}\")\n",
    "        raise\n",
    "\n",
    "def drop_columns(df, columns_to_drop=None, include_missing_log=True):\n",
    "    \"\"\"\n",
    "    Drop specified columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "    columns_to_drop (list or None): List of columns to drop. If None or empty, no columns are dropped.\n",
    "    include_missing_log (bool): Whether to log columns that are not found in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame after removing the specified columns.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the input `df` is not a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input DataFrame\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input `df` must be a pandas DataFrame.\")\n",
    "\n",
    "        # Handle None or empty column list\n",
    "        if not columns_to_drop:\n",
    "            logging.info(\"No columns specified for dropping. Returning the original DataFrame.\")\n",
    "            return df\n",
    "\n",
    "        # Identify columns to actually drop\n",
    "        columns_present = [col for col in columns_to_drop if col in df.columns]\n",
    "        columns_missing = [col for col in columns_to_drop if col not in df.columns]\n",
    "\n",
    "        # Log missing columns if enabled\n",
    "        if include_missing_log and columns_missing:\n",
    "            logging.warning(f\"The following columns were not found in the dataset: {columns_missing}\")\n",
    "\n",
    "        # Drop columns that exist in the DataFrame\n",
    "        if columns_present:\n",
    "            logging.info(f\"Removing the following columns: {columns_present}\")\n",
    "            df = df.drop(columns=columns_present)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"ValueError in drop_columns: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in drop_columns: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_data(df):\n",
    "    \"\"\"\n",
    "    Validate the dataset's integrity and check for anomalies.\n",
    "\n",
    "    Actions:\n",
    "    - Check for negative values in 'Monthly_Total' and 'Monthly_Average'.\n",
    "    - Log the counts of negative values for each column.\n",
    "    - Perform basic data integrity checks (e.g., missing columns, NaNs, infinite values).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: Validation summary containing counts of negative values, missing values, and other anomalies.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If required columns are missing or other critical issues are found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input `df` must be a pandas DataFrame.\")\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        required_columns = ['Monthly_Total', 'Monthly_Average']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        # Initialize validation summary\n",
    "        validation_summary = {}\n",
    "\n",
    "        # Check for negative values\n",
    "        validation_summary['Negative Monthly_Total Values'] = df[df['Monthly_Total'] < 0].shape[0]\n",
    "        validation_summary['Negative Monthly_Average Values'] = df[df['Monthly_Average'] < 0].shape[0]\n",
    "\n",
    "        # Check for missing values\n",
    "        validation_summary['Missing Values'] = df.isnull().sum().sum()\n",
    "\n",
    "        # Check for infinite values in numeric columns only\n",
    "        numeric_df = df.select_dtypes(include=[np.number])\n",
    "        validation_summary['Infinite Values'] = np.isinf(numeric_df.values).sum()\n",
    "\n",
    "        # Log validation summary\n",
    "        logging.info(\"Dataset Validation Summary:\")\n",
    "        for key, value in validation_summary.items():\n",
    "            logging.info(f\"{key}: {value}\")\n",
    "\n",
    "        return validation_summary\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in validate_data: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in validate_data: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in validate_data: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_yearly_trends_with_metrics(data, metrics, title_prefix=\"Yearly Trends\"):\n",
    "    \"\"\"\n",
    "    Create an interactive plot for yearly trends using Plotly with anomaly detection and clustering.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Aggregated yearly data containing metrics to plot.\n",
    "    metrics (list of dict): List of dictionaries where each defines:\n",
    "        - \"column\": Column name in the data to plot.\n",
    "        - \"title\": Title for the metric (e.g., 'Yearly Total Rainfall').\n",
    "        - \"color\": Color for the line plot.\n",
    "    title_prefix (str): Prefix for the overall plot title.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional anomaly and cluster labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input data\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"The `data` parameter must be a pandas DataFrame.\")\n",
    "        if not all(\"column\" in metric and \"title\" in metric and \"color\" in metric for metric in metrics):\n",
    "            raise ValueError(\"Each metric in `metrics` must include 'column', 'title', and 'color'.\")\n",
    "\n",
    "        # Check required columns in data\n",
    "        required_columns = [metric['column'] for metric in metrics] + ['Year']\n",
    "        missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Missing required columns in data: {missing_columns}\")\n",
    "\n",
    "        # Z-score-based anomaly detection for each metric\n",
    "        for metric in metrics:\n",
    "            column = metric[\"column\"]\n",
    "            data[f\"{column}_Zscore\"] = zscore(data[column])\n",
    "            data[f\"{column}_Anomaly\"] = abs(data[f\"{column}_Zscore\"]) > 2  # Mark anomalies where |Z-score| > 2\n",
    "\n",
    "        # Clustering analysis (K-Means) based on selected metrics\n",
    "        clustering_features = [metric['column'] for metric in metrics]\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        data['Cluster'] = kmeans.fit_predict(data[clustering_features])\n",
    "\n",
    "        # Create a subplot for each metric\n",
    "        fig = make_subplots(\n",
    "            rows=len(metrics) + 1,  # Extra row for cluster analysis\n",
    "            cols=1,\n",
    "            shared_xaxes=True,\n",
    "            subplot_titles=[m[\"title\"] for m in metrics] + [\"Rainfall Clustering\"]\n",
    "        )\n",
    "\n",
    "        for idx, metric in enumerate(metrics, start=1):\n",
    "            column = metric[\"column\"]\n",
    "            title = metric[\"title\"]\n",
    "            color = metric[\"color\"]\n",
    "\n",
    "            # Add trace for each metric with anomaly highlighting\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=data['Year'],\n",
    "                    y=data[column],\n",
    "                    mode='lines+markers',\n",
    "                    name=title,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=idx, col=1\n",
    "            )\n",
    "            # Highlight anomalies\n",
    "            anomalies = data[data[f\"{column}_Anomaly\"]]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=anomalies['Year'],\n",
    "                    y=anomalies[column],\n",
    "                    mode='markers',\n",
    "                    name=f\"{title} Anomalies\",\n",
    "                    marker=dict(color='red', size=10, symbol='x')\n",
    "                ),\n",
    "                row=idx, col=1\n",
    "            )\n",
    "\n",
    "        # Add clustering results\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=data['Year'],\n",
    "                y=data['Cluster'],\n",
    "                mode='markers',\n",
    "                name=\"Clusters\",\n",
    "                marker=dict(color=data['Cluster'], size=10, colorscale='Viridis')\n",
    "            ),\n",
    "            row=len(metrics) + 1, col=1\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"{title_prefix}: {data.index.min().year} - {data.index.max().year}\",\n",
    "            xaxis_title=\"Year\",\n",
    "            height=400 * (len(metrics) + 1),  # Adjust height dynamically\n",
    "            showlegend=True,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        fig.update_xaxes(tickformat=\"%Y\")\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "\n",
    "        return data  # Return data with additional metrics\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in plot_yearly_trends_with_metrics: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in plot_yearly_trends_with_metrics: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in plot_yearly_trends_with_metrics: {e}\")\n",
    "        raise\n",
    "\n",
    "def evaluate_model(y_actual, y_pred, metrics_to_compute=None):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions with a variety of metrics.\n",
    "\n",
    "    Parameters:\n",
    "        y_actual (np.ndarray): True target values.\n",
    "        y_pred (np.ndarray): Predicted values.\n",
    "        metrics_to_compute (list): List of metrics to compute. Default includes MAE, MSE, R², SMAPE, RMSE, MAPE, Explained Variance, and MBE.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        if len(y_actual) != len(y_pred):\n",
    "            raise ValueError(\"Length of `y_actual` and `y_pred` must match.\")\n",
    "        \n",
    "        if metrics_to_compute is None:\n",
    "            metrics_to_compute = [\n",
    "                \"MAE\", \"MSE\", \"R²\", \"SMAPE\", \"RMSE\", \"MAPE\", \n",
    "                \"Explained Variance\", \"MBE\"\n",
    "            ]\n",
    "\n",
    "        # Initialize metrics dictionary\n",
    "        metrics = {}\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        if \"MAE\" in metrics_to_compute:\n",
    "            metrics[\"Mean Absolute Error (MAE)\"] = mean_absolute_error(y_actual, y_pred)\n",
    "        \n",
    "        # Mean Squared Error (MSE)\n",
    "        if \"MSE\" in metrics_to_compute:\n",
    "            metrics[\"Mean Squared Error (MSE)\"] = mean_squared_error(y_actual, y_pred)\n",
    "        \n",
    "        # R² Score\n",
    "        if \"R²\" in metrics_to_compute:\n",
    "            metrics[\"R² Score\"] = r2_score(y_actual, y_pred)\n",
    "        \n",
    "        # Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "        if \"SMAPE\" in metrics_to_compute:\n",
    "            smape = np.mean(2 * np.abs(y_actual - y_pred) / \n",
    "                            (np.abs(y_actual) + np.abs(y_pred) + 1e-10)) * 100\n",
    "            metrics[\"Symmetric Mean Absolute Percentage Error (SMAPE)\"] = smape\n",
    "        \n",
    "        # Root Mean Squared Error (RMSE)\n",
    "        if \"RMSE\" in metrics_to_compute:\n",
    "            metrics[\"Root Mean Squared Error (RMSE)\"] = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "        \n",
    "        # Mean Absolute Percentage Error (MAPE)\n",
    "        if \"MAPE\" in metrics_to_compute:\n",
    "            mape = np.mean(np.abs((y_actual - y_pred) / (y_actual + 1e-10))) * 100\n",
    "            metrics[\"Mean Absolute Percentage Error (MAPE)\"] = mape\n",
    "        \n",
    "        # Explained Variance Score\n",
    "        if \"Explained Variance\" in metrics_to_compute:\n",
    "            explained_variance = 1 - np.var(y_actual - y_pred) / np.var(y_actual)\n",
    "            metrics[\"Explained Variance\"] = explained_variance\n",
    "        \n",
    "        # Mean Bias Error (MBE)\n",
    "        if \"MBE\" in metrics_to_compute:\n",
    "            mbe = np.mean(y_pred - y_actual)\n",
    "            metrics[\"Mean Bias Error (MBE)\"] = mbe\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in evaluate_model: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in evaluate_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_distributions(\n",
    "    df,\n",
    "    columns_to_plot=None,\n",
    "    plot_type=\"both\",\n",
    "    bins=30,\n",
    "    kde_bw_adjust=1.0,\n",
    "    save_plots=False,\n",
    "    output_path=\"plots/\",\n",
    "    dataset_type=\"Original\",\n",
    "    model_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the distributions of rainfall metrics with dynamic analysis and evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataset.\n",
    "    columns_to_plot (list): List of column names to plot. Defaults to 'Monthly_Total' and 'Monthly_Average'.\n",
    "    plot_type (str): Type of plot ('hist', 'kde', or 'both'). Default is 'both'.\n",
    "    bins (int): Number of bins for the histogram. Default is 30.\n",
    "    kde_bw_adjust (float): Bandwidth adjustment for KDE. Default is 1.0.\n",
    "    save_plots (bool): Whether to save plots to a directory. Default is False.\n",
    "    output_path (str): Directory path to save plots. Default is \"plots/\".\n",
    "    dataset_type (str): Type of dataset (e.g., \"Original\", \"Training\", \"Predicted\").\n",
    "    model_name (str): Name of the model for visualization. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    dict: Evaluation metrics for each column plotted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Default columns to plot\n",
    "        if columns_to_plot is None:\n",
    "            columns_to_plot = [\"Monthly_Total\", \"Monthly_Average\"]\n",
    "\n",
    "        # Ensure columns exist in DataFrame\n",
    "        missing_columns = [col for col in columns_to_plot if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Missing required columns for plotting: {missing_columns}\")\n",
    "\n",
    "        # Initialize metrics dictionary\n",
    "        all_metrics = {}\n",
    "\n",
    "        sns.set(style=\"whitegrid\")\n",
    "\n",
    "        # Iterate through columns and plot distributions\n",
    "        for column in columns_to_plot:\n",
    "            # Ensure the column has valid data\n",
    "            if df[column].isna().all():\n",
    "                raise ValueError(f\"Column '{column}' contains no valid data to plot.\")\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                \"Mean\": df[column].mean(),\n",
    "                \"Median\": df[column].median(),\n",
    "                \"Skewness\": df[column].skew(),\n",
    "                \"Kurtosis\": df[column].kurt(),\n",
    "                \"Min\": df[column].min(),\n",
    "                \"Max\": df[column].max(),\n",
    "                \"Standard Deviation\": df[column].std(),\n",
    "            }\n",
    "            all_metrics[column] = metrics\n",
    "\n",
    "            # Print metrics for the column\n",
    "            logging.info(f\"\\nMetrics for {column} ({dataset_type}, Model: {model_name or 'N/A'}):\")\n",
    "            for metric, value in metrics.items():\n",
    "                logging.info(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "            # Plotting\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            if plot_type in [\"hist\", \"both\"]:\n",
    "                sns.histplot(df[column], kde=False, bins=bins, label=\"Histogram\", color=\"blue\", alpha=0.7)\n",
    "            if plot_type in [\"kde\", \"both\"]:\n",
    "                sns.kdeplot(df[column], bw_adjust=kde_bw_adjust, label=\"KDE\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "            # Customize title\n",
    "            plot_title = f\"Distribution of {column} ({dataset_type})\"\n",
    "            if model_name:\n",
    "                plot_title += f\" - {model_name}\"\n",
    "            plt.title(plot_title)\n",
    "            plt.xlabel(f\"{column} (mm)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save plot if required\n",
    "            if save_plots:\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "                plot_filename = f\"{output_path}{dataset_type}_{model_name or 'General'}_{column}_distribution.png\"\n",
    "                plt.savefig(plot_filename)\n",
    "                logging.info(f\"Plot saved to: {plot_filename}\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        return all_metrics\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in plot_distributions: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in plot_distributions: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in plot_distributions: {e}\")\n",
    "        raise\n",
    "\n",
    "def handle_file(file_input=None, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Handle file input dynamically and ensure it works across environments.\n",
    "\n",
    "    Parameters:\n",
    "        file_input (str, FileUpload, or None): \n",
    "            - If `str`: Path to the input file (for backend or CLI environments).\n",
    "            - If `FileUpload`: Jupyter Notebook's interactive file upload object.\n",
    "            - If `None`: Automatically prompt for file upload in a notebook environment.\n",
    "        output_dir (str): Directory to save processed files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame from the input file.\n",
    "        str: Path to save the processed output file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if isinstance(file_input, str):\n",
    "            # Case 1: File path provided as string (Backend/CLI)\n",
    "            if not os.path.exists(file_input):\n",
    "                raise FileNotFoundError(f\"File not found at: {file_input}\")\n",
    "            df = pd.read_csv(file_input)\n",
    "            output_file = os.path.join(output_dir, f\"processed_{os.path.basename(file_input)}\")\n",
    "            logging.info(f\"File loaded successfully from path: {file_input}\")\n",
    "            return df, output_file\n",
    "\n",
    "        elif isinstance(file_input, FileUpload):\n",
    "            # Case 2: Interactive file upload in Jupyter Notebook\n",
    "            uploaded_file = list(file_input.value.values())[0]\n",
    "            content = uploaded_file[\"content\"]\n",
    "            file_name = uploaded_file[\"metadata\"][\"name\"]\n",
    "            df = pd.read_csv(io.StringIO(content.decode(\"utf-8\")))\n",
    "            output_file = os.path.join(output_dir, f\"processed_{file_name}\")\n",
    "            logging.info(f\"File uploaded successfully via Jupyter widget: {file_name}\")\n",
    "            return df, output_file\n",
    "\n",
    "        elif file_input is None:\n",
    "            # Case 3: Automatically launch file upload widget in Jupyter Notebook\n",
    "            print(\"Please upload a file:\")\n",
    "            upload_widget = FileUpload(accept=\".csv\", multiple=False)\n",
    "            display(upload_widget)\n",
    "\n",
    "            # Wait for file upload with timeout\n",
    "            import time\n",
    "            timeout = 60  # Set timeout to 60 seconds\n",
    "            start_time = time.time()\n",
    "\n",
    "            while not upload_widget.value:\n",
    "                if time.time() - start_time > timeout:\n",
    "                    raise TimeoutError(\"File upload timed out. Please try again.\")\n",
    "                time.sleep(0.5)  # Prevent busy-waiting\n",
    "\n",
    "            logging.info(\"File uploaded via Jupyter widget and detected successfully.\")\n",
    "\n",
    "            # Process the uploaded file without recursion\n",
    "            uploaded_file = list(upload_widget.value.values())[0]\n",
    "            content = uploaded_file[\"content\"]\n",
    "            file_name = uploaded_file[\"metadata\"][\"name\"]\n",
    "            df = pd.read_csv(io.StringIO(content.decode(\"utf-8\")))\n",
    "            output_file = os.path.join(output_dir, f\"processed_{file_name}\")\n",
    "            return df, output_file\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file input type. Provide a valid file path or upload widget.\")\n",
    "\n",
    "    except TimeoutError as te:\n",
    "        logging.error(f\"TimeoutError: {te}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error handling file: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_month_column(df):\n",
    "    \"\"\"\n",
    "    Create a 'Month' column using 'YEAR' and 'Month' columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing 'YEAR' and 'Month'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a 'Month' column in datetime format.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If 'YEAR' or 'Month' columns are missing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'YEAR' in df.columns and 'Month' in df.columns:\n",
    "            df['Month'] = pd.to_datetime(df[['YEAR', 'Month']].assign(day=1), errors='coerce')\n",
    "            if df['Month'].isna().any():\n",
    "                raise ValueError(\"Some 'Month' values could not be converted to datetime.\")\n",
    "            return df\n",
    "        else:\n",
    "            raise KeyError(\"Missing 'YEAR' or 'Month' columns required for datetime conversion.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create 'Month' column: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_year_month(df):\n",
    "    \"\"\"\n",
    "    Validate and clean YEAR and Month columns for datetime conversion.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with cleaned YEAR and Month columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate YEAR column\n",
    "        if 'YEAR' in df.columns:\n",
    "            invalid_years = df[~df['YEAR'].apply(lambda x: isinstance(x, (int, float)) and 1800 <= x <= 2100)]\n",
    "            if not invalid_years.empty:\n",
    "                logging.warning(f\"Invalid YEAR entries found: {invalid_years['YEAR'].unique()}\")\n",
    "            df['YEAR'] = pd.to_numeric(df['YEAR'], errors='coerce').astype('Int64')  # Ensure YEAR remains integers\n",
    "        \n",
    "        # Validate and convert Month column to datetime\n",
    "        if 'Month' in df.columns:\n",
    "            invalid_months = df[~df['Month'].apply(lambda x: isinstance(x, (int, float)) and 1 <= x <= 12)]\n",
    "            if not invalid_months.empty:\n",
    "                logging.warning(f\"Invalid Month entries found: {invalid_months['Month'].unique()}\")\n",
    "            df['Month'] = pd.to_numeric(df['Month'], errors='coerce')  # Ensure Month is numeric\n",
    "\n",
    "            # Combine YEAR and Month into a datetime column\n",
    "            df['Month'] = pd.to_datetime(df[['YEAR', 'Month']].assign(day=1), errors='coerce')\n",
    "\n",
    "        # Log final validation state\n",
    "        logging.info(f\"YEAR column type: {df['YEAR'].dtype}\")\n",
    "        logging.info(f\"Month column type: {df['Month'].dtype}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in validate_year_month: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_dataset(df, output_path, save_file=False):\n",
    "    \"\"\"\n",
    "    Optionally save the cleaned dataset to a specified file path and provide a download link.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataset to save.\n",
    "    output_path (str): Path to save the dataset, including the file name.\n",
    "    save_file (bool): Whether to save the dataset to a file. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    str: Path to the saved file or a message indicating it was not saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not save_file:\n",
    "            logging.info(\"File saving is skipped by user preference.\")\n",
    "            return \"File saving skipped.\"\n",
    "\n",
    "        # Validate input parameters\n",
    "        if not isinstance(output_path, str) or not output_path.endswith('.csv'):\n",
    "            raise ValueError(\"Output path must be a valid CSV file path.\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"The dataset is empty. Nothing to save.\")\n",
    "\n",
    "        # Save the dataset\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Processed dataset saved successfully at {output_path}\")\n",
    "\n",
    "        # Provide download link in Jupyter Notebook\n",
    "        if \"IPython\" in globals():\n",
    "            return FileLink(output_path)\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    except PermissionError:\n",
    "        logging.error(f\"Permission denied: Cannot save to {output_path}. Check write permissions.\")\n",
    "        raise\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"FileNotFoundError: The directory for {output_path} does not exist.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in save_dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ----------------------- Main Workflow ----------------------- #\n",
    "\n",
    "#Step 1: Load the dataset\n",
    "\n",
    "file_path = \"/Users/feysel/Developer/Practices/Python/refined/precipitation_data.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())\n",
    "\n",
    "#Step 1.1: Create 'Month' column\n",
    "df = create_month_column(df)\n",
    "logging.info(\"Created 'Month' column successfully.\")\n",
    "\n",
    "# try:\n",
    "#     # Choose the file input method\n",
    "#     # If you want to use a static file path, set it here\n",
    "#     static_file_path = \"/Users/feysel/Developer/Practices/Python/refined/precipitation_data.csv\"  # Replace with the actual path\n",
    "\n",
    "#     if static_file_path:\n",
    "#         # Use the static file path if specified\n",
    "#         input_file = static_file_path\n",
    "#         logging.info(f\"Using static file path: {static_file_path}\")\n",
    "#     else:\n",
    "#         # Default to the dynamic file handling function\n",
    "#         if 'get_ipython' not in globals():\n",
    "#             # CLI environment: prompt for a file path\n",
    "#             input_file = input(\"Enter the path to your dataset (CSV format): \").strip()\n",
    "#             if not os.path.exists(input_file):\n",
    "#                 raise FileNotFoundError(f\"File not found at the specified path: {input_file}\")\n",
    "#         else:\n",
    "#             # Jupyter Notebook environment: use upload widget\n",
    "#             input_file = None  # This triggers the upload widget in handle_file\n",
    "\n",
    "#     # Load the dataset\n",
    "#     df, output_file = handle_file(file_input=input_file)\n",
    "#     if df.empty:\n",
    "#         raise ValueError(\"The loaded dataset is empty. Aborting further steps.\")\n",
    "    \n",
    "#     logging.info(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "# except FileNotFoundError as e:\n",
    "#     logging.error(f\"File error: {e}\")\n",
    "#     raise\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Unexpected error while handling the file: {e}\")\n",
    "#     raise\n",
    "\n",
    "\n",
    "#Step 2: Handle missing values and redundant columns\n",
    "logging.info(\"Handling missing values...\")\n",
    "df = handle_missing_values(df, drop_cells=True, drop_rows=False, threshold=0.5)\n",
    "if 'Month' not in df.columns:\n",
    "    df = create_month_column(df)  # Recreate if dropped\n",
    "logging.info(f\"Missing values handled. Dataset now has {df.isna().sum().sum()} missing cells.\")\n",
    "\n",
    "\n",
    "print(\"Columns in DataFrame:\", df.columns)\n",
    "\n",
    "# Step 3: Extract Features and Metrics\n",
    "logging.info(\"Performing feature engineering...\")\n",
    "try:\n",
    "    df, metadata, daily_columns = extract_features_and_metrics(\n",
    "        df,\n",
    "        daily_columns_prefix=\"\",  # Leave blank if no specific prefix exists\n",
    "        rolling_window=12,\n",
    "        validate_values=True\n",
    "    )\n",
    "    logging.info(f\"Feature engineering completed. Metadata: {metadata}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during feature extraction: {e}\")\n",
    "    raise\n",
    "\n",
    "#Step 4: Remove redundant columns\n",
    "logging.info(\"Removing redundant columns...\")\n",
    "df = drop_columns(df, columns_to_drop=daily_columns)\n",
    "logging.info(f\"Redundant columns removed. Dataset now has {df.shape[1]} columns.\")\n",
    "\n",
    "#Step 5: Validate the dataset\n",
    "logging.info(\"Validating the dataset...\")\n",
    "validation_summary = validate_data(df)\n",
    "if validation_summary['Missing Values'] > 0:\n",
    "    logging.warning(\"Dataset contains missing values after handling. Review required.\")\n",
    "logging.info(f\"Validation completed. Summary: {validation_summary}\")\n",
    "\n",
    "#Step 6: Visualize distributions\n",
    "logging.info(\"Visualizing distributions...\")\n",
    "plot_metrics = plot_distributions(\n",
    "    df, \n",
    "    columns_to_plot=[\"Monthly_Total\", \"Monthly_Average\"], \n",
    "    plot_type=\"both\", \n",
    "    save_plots=True, \n",
    "    output_path=\"plots/\",\n",
    "    dataset_type=\"Processed\"\n",
    ")\n",
    "logging.info(f\"Distribution visualizations saved successfully.\")\n",
    "\n",
    "# Step 7: Aggregate data by year with new metrics\n",
    "try:\n",
    "    # Log columns for debugging\n",
    "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    \n",
    "    # Validate time series index\n",
    "    if 'Month' in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.set_index('Month', inplace=True)\n",
    "\n",
    "    # Resample by year using the updated 'YE' frequency\n",
    "    yearly_data = df.resample('YE').agg({\n",
    "        'Monthly_Total': 'sum',  # Total rainfall per year\n",
    "        'Monthly_Average': 'mean'  # Average monthly rainfall per year\n",
    "    })\n",
    "\n",
    "    # Add 'Year' column for plotting, ensuring it is datetime\n",
    "    yearly_data['Year'] = yearly_data.index.year.astype(int)\n",
    "\n",
    "    # Validate yearly data for issues\n",
    "    if yearly_data.isnull().any().any():\n",
    "        logging.warning(\"Yearly data contains missing values.\")\n",
    "    if (yearly_data[['Monthly_Total', 'Monthly_Average']] <= 0).any().any():\n",
    "        logging.warning(\"Yearly data contains zero or negative values.\")\n",
    "\n",
    "    # Define metrics for interactive plotting\n",
    "    metrics_to_plot = [\n",
    "        {\"column\": \"Monthly_Total\", \"title\": \"Yearly Total Rainfall\", \"color\": \"blue\"},\n",
    "        {\"column\": \"Monthly_Average\", \"title\": \"Yearly Average Rainfall\", \"color\": \"green\"}\n",
    "    ]\n",
    "\n",
    "    # Plot enhanced interactive yearly trends\n",
    "    plot_yearly_trends_interactive(yearly_data, metrics_to_plot)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to aggregate data by year: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 8: Save the processed dataset (optional)\n",
    "save_file = False  # Set to False to skip saving\n",
    "if save_file:\n",
    "    logging.info(\"Saving the processed dataset...\")\n",
    "    save_result = save_dataset(df, output_file, save_file=save_file)\n",
    "    if isinstance(save_result, FileLink):\n",
    "        display(save_result)  # Display download link in Jupyter\n",
    "    logging.info(f\"Processed dataset saved to {output_file}.\")\n",
    "else:\n",
    "    logging.info(\"Skipping dataset save as per user preference.\")\n",
    "\n",
    "# --------------------------------- 1: DATA VISUALIZATION -------------------------------- #\n",
    "def load_visualization_data(file_path=None, df=None):\n",
    "    \"\"\"\n",
    "    Load the processed dataset for visualization.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): File path to load the processed dataset.\n",
    "    df (pd.DataFrame or None): Optionally provide a preloaded DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Processed DataFrame ready for visualization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        if df is not None:\n",
    "            logging.info(\"Using provided DataFrame for visualization.\")\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                raise ValueError(\"Provided `df` is not a valid DataFrame.\")\n",
    "            return df\n",
    "\n",
    "        if file_path is None:\n",
    "            raise ValueError(\"Either `file_path` or `df` must be provided.\")\n",
    "\n",
    "        # Load the dataset dynamically\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"The dataset at {file_path} is empty.\")\n",
    "\n",
    "        # Ensure 'Month' column is in datetime format\n",
    "        if 'Month' not in df.columns:\n",
    "            raise KeyError(\"'Month' column is missing in the dataset.\")\n",
    "        df['Month'] = pd.to_datetime(df['Month'])\n",
    "\n",
    "        logging.info(f\"Dataset for visualization successfully loaded from: {file_path}\")\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found at: {file_path}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"ValueError in load_visualization_data: {ve}\")\n",
    "        raise\n",
    "    except KeyError as ke:\n",
    "        logging.error(f\"KeyError in load_visualization_data: {ke}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in load_visualization_data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# -------------------- STEP 1: TIME-SERIES PLOTS -------------------- #\n",
    "def plot_time_series(\n",
    "    df,\n",
    "    column_to_plot=\"Monthly_Total\",\n",
    "    rolling_column=\"Rolling_Average\",\n",
    "    time_span_years=5,\n",
    "    figsize=(16, 8),\n",
    "    save_plots=False,\n",
    "    output_path=\"plots/\",\n",
    "    interactive=False,\n",
    "    include_metrics=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize rainfall trends across specific time spans and the entire timeline.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data.\n",
    "    column_to_plot (str): Column name for the main metric to plot (e.g., Monthly_Total).\n",
    "    rolling_column (str): Column name for the rolling average to overlay.\n",
    "    time_span_years (int): Number of years for each time-span subset.\n",
    "    figsize (tuple): Figure size for plots.\n",
    "    save_plots (bool): Whether to save the plots to files.\n",
    "    output_path (str): Directory to save the plots if enabled.\n",
    "    interactive (bool): Whether to create interactive plots using Plotly.\n",
    "    include_metrics (bool): Whether to include aggregate metrics on the plots.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input DataFrame\n",
    "        if column_to_plot not in df.columns or rolling_column not in df.columns:\n",
    "            raise KeyError(f\"Columns {column_to_plot} or {rolling_column} not found in DataFrame.\")\n",
    "\n",
    "        # Ensure 'Month' column is in datetime format\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['Month']):\n",
    "            raise ValueError(\"The 'Month' column must be a datetime type.\")\n",
    "\n",
    "        # Calculate time spans dynamically\n",
    "        min_year = df['Month'].dt.year.min()\n",
    "        max_year = df['Month'].dt.year.max()\n",
    "        total_years = max_year - min_year + 1\n",
    "        subsets_count = (total_years + time_span_years - 1) // time_span_years  # Round up\n",
    "\n",
    "        for i in range(subsets_count):\n",
    "            start_year = min_year + i * time_span_years\n",
    "            end_year = min(start_year + time_span_years - 1, max_year)\n",
    "\n",
    "            # Subset data\n",
    "            subset = df[(df['Month'].dt.year >= start_year) & (df['Month'].dt.year <= end_year)]\n",
    "            if subset.empty:\n",
    "                logging.warning(f\"No data found for the time span: {start_year} - {end_year}\")\n",
    "                continue\n",
    "\n",
    "            # Initialize the plot\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "            # Plot the main metric and rolling average\n",
    "            plt.plot(subset['Month'], subset[column_to_plot], label=column_to_plot, color='blue')\n",
    "            plt.plot(subset['Month'], subset[rolling_column], label=f\"{rolling_column} (Rolling)\", color='green')\n",
    "\n",
    "            # Include metrics if enabled\n",
    "            if include_metrics:\n",
    "                total_mean = subset[column_to_plot].mean()\n",
    "                total_median = subset[column_to_plot].median()\n",
    "                plt.axhline(total_mean, color='red', linestyle='--', linewidth=1, label=f'Mean: {total_mean:.2f}')\n",
    "                plt.axhline(total_median, color='orange', linestyle='--', linewidth=1, label=f'Median: {total_median:.2f}')\n",
    "\n",
    "            # Customize x-axis for legibility\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display years only\n",
    "            plt.gca().xaxis.set_major_locator(mdates.YearLocator(1))  # Major ticks every year\n",
    "            plt.gca().xaxis.set_minor_locator(mdates.MonthLocator())  # Minor ticks every month\n",
    "            plt.grid(which='minor', linestyle=':', linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "            # Add titles and labels\n",
    "            plt.title(f\"Rainfall Trends: {start_year} to {end_year}\")\n",
    "            plt.xlabel(\"Year\")\n",
    "            plt.ylabel(\"Rainfall (mm)\")\n",
    "            plt.legend()\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot if required\n",
    "            if save_plots:\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "                plot_filename = f\"{output_path}rainfall_trend_{start_year}_{end_year}.png\"\n",
    "                plt.savefig(plot_filename)\n",
    "                logging.info(f\"Plot saved to: {plot_filename}\")\n",
    "\n",
    "            # Show the plot\n",
    "            if interactive:\n",
    "                import plotly.express as px\n",
    "                fig = px.line(subset, x='Month', y=[column_to_plot, rolling_column],\n",
    "                              labels={\"value\": \"Rainfall (mm)\", \"variable\": \"Metric\"},\n",
    "                              title=f\"Rainfall Trends ({start_year}-{end_year})\")\n",
    "                fig.show()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "        # Plot the entire timeline\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(df['Month'], df[column_to_plot], label=column_to_plot, color='blue')\n",
    "        plt.plot(df['Month'], df[rolling_column], label=f\"{rolling_column} (Rolling)\", color='green')\n",
    "\n",
    "        # Include metrics if enabled\n",
    "        if include_metrics:\n",
    "            total_mean = df[column_to_plot].mean()\n",
    "            total_median = df[column_to_plot].median()\n",
    "            plt.axhline(total_mean, color='red', linestyle='--', linewidth=1, label=f'Mean: {total_mean:.2f}')\n",
    "            plt.axhline(total_median, color='orange', linestyle='--', linewidth=1, label=f'Median: {total_median:.2f}')\n",
    "\n",
    "        plt.title(\"Rainfall Trends: Full Timeline\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Rainfall (mm)\")\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot if required\n",
    "        if save_plots:\n",
    "            full_timeline_filename = f\"{output_path}rainfall_trend_full_timeline.png\"\n",
    "            plt.savefig(full_timeline_filename)\n",
    "            logging.info(f\"Full timeline plot saved to: {full_timeline_filename}\")\n",
    "\n",
    "        # Show the plot\n",
    "        if interactive:\n",
    "            fig = px.line(df, x='Month', y=[column_to_plot, rolling_column],\n",
    "                          labels={\"value\": \"Rainfall (mm)\", \"variable\": \"Metric\"},\n",
    "                          title=\"Rainfall Trends: Full Timeline\")\n",
    "            fig.show()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in plot_time_series: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in plot_time_series: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in plot_time_series: {e}\")\n",
    "        raise\n",
    "\n",
    "# -------------------- STEP 2: YEARLY TRENDS -------------------- #\n",
    "def plot_yearly_trends_interactive(data, metrics, title_prefix=\"Yearly Trends\"):\n",
    "    \"\"\"\n",
    "    Create an interactive plot for yearly trends using Plotly with robust error handling.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Aggregated yearly data containing metrics to plot.\n",
    "    metrics (list of dict): List of dictionaries where each defines:\n",
    "        - \"column\": Column name in the data to plot.\n",
    "        - \"title\": Title for the metric (e.g., 'Yearly Total Rainfall').\n",
    "        - \"color\": Color for the line plot.\n",
    "    title_prefix (str): Prefix for the overall plot title.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input data\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"The `data` parameter must be a pandas DataFrame.\")\n",
    "        if not all(\"column\" in metric and \"title\" in metric and \"color\" in metric for metric in metrics):\n",
    "            raise ValueError(\"Each metric in `metrics` must include 'column', 'title', and 'color'.\")\n",
    "\n",
    "        # Check required columns in data\n",
    "        required_columns = [metric['column'] for metric in metrics] + ['Year']\n",
    "        missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Missing required columns in data: {missing_columns}\")\n",
    "\n",
    "        # Check for invalid or missing data\n",
    "        if data.isnull().any().any():\n",
    "            logging.warning(\"The data contains missing values. Please investigate.\")\n",
    "    \n",
    "        # Validate for negative values in numeric columns only\n",
    "        numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "        for column in numeric_columns:\n",
    "            if (data[column] < 0).any():\n",
    "                logging.warning(f\"The column '{column}' contains negative values. Ensure data validity.\")\n",
    "\n",
    "        # Create a subplot for each metric\n",
    "        fig = make_subplots(rows=len(metrics), cols=1, shared_xaxes=True, subplot_titles=[m[\"title\"] for m in metrics])\n",
    "\n",
    "        for idx, metric in enumerate(metrics, start=1):\n",
    "            column = metric[\"column\"]\n",
    "            title = metric[\"title\"]\n",
    "            color = metric[\"color\"]\n",
    "\n",
    "            # Add trace for each metric\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=data['Year'],\n",
    "                    y=data[column],\n",
    "                    mode='lines+markers',\n",
    "                    name=title,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=idx, col=1\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"{title_prefix}: {data['Year'].min()} - {data['Year'].max()}\",\n",
    "            xaxis_title=\"Year\",\n",
    "            height=300 * len(metrics),\n",
    "            showlegend=True,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        fig.update_xaxes(tickformat=\"%Y\")\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in plot_yearly_trends_interactive: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in plot_yearly_trends_interactive: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in plot_yearly_trends_interactive: {e}\")\n",
    "        raise\n",
    "\n",
    "# Validate time series index\n",
    "if 'Month' in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
    "    df.set_index('Month', inplace=True)\n",
    "\n",
    "# Aggregate data by year\n",
    "try:\n",
    "    yearly_data = df.resample('Y').agg({\n",
    "        'Monthly_Total': 'sum',  # Total rainfall per year\n",
    "        'Monthly_Average': 'mean'  # Average monthly rainfall per year\n",
    "    })\n",
    "\n",
    "    # Add 'Year' column for plotting\n",
    "    yearly_data['Year'] = yearly_data.index\n",
    "\n",
    "    # Validate yearly data for issues\n",
    "    if yearly_data.isnull().any().any():\n",
    "        logging.warning(\"Yearly data contains missing values.\")\n",
    "    if (yearly_data[['Monthly_Total', 'Monthly_Average']] <= 0).any().any():\n",
    "        logging.warning(\"Yearly data contains zero or negative values.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during yearly data aggregation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define metrics for interactive plotting\n",
    "metrics_to_plot = [\n",
    "    {\"column\": \"Monthly_Total\", \"title\": \"Yearly Total Rainfall\", \"color\": \"blue\"},\n",
    "    {\"column\": \"Monthly_Average\", \"title\": \"Yearly Average Rainfall\", \"color\": \"green\"}\n",
    "]\n",
    "\n",
    "# Plot enhanced interactive yearly trends\n",
    "try:\n",
    "    plot_yearly_trends_interactive(yearly_data, metrics_to_plot)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to plot yearly trends: {e}\")\n",
    "    raise\n",
    "\n",
    "# -------------------- STEP 3: INTERACTIVE BOX PLOTS -------------------- #\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_boxplots_interactive(data, grouping_column, value_column, title, x_labels=None, annotation=None, color='lightblue'):\n",
    "    \"\"\"\n",
    "    Create an interactive box plot using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the data to plot.\n",
    "    grouping_column (str): Column to group data by (e.g., 'Year', 'Month_Number').\n",
    "    value_column (str): Column containing the values to plot.\n",
    "    title (str): Title of the box plot.\n",
    "    x_labels (list, optional): Custom labels for the x-axis (e.g., month names for 'Month_Number').\n",
    "    annotation (str, optional): Annotation text to describe the box plot features.\n",
    "    color (str): Color of the box plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if grouping_column not in data.columns or value_column not in data.columns:\n",
    "            raise KeyError(f\"Columns '{grouping_column}' or '{value_column}' are missing in the dataset.\")\n",
    "\n",
    "        if data[value_column].isnull().any():\n",
    "            logging.warning(f\"Column '{value_column}' contains missing values.\")\n",
    "\n",
    "        # Create a box plot\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Group data for the box plot\n",
    "        for group in sorted(data[grouping_column].unique()):\n",
    "            group_data = data[data[grouping_column] == group][value_column]\n",
    "            fig.add_trace(go.Box(\n",
    "                y=group_data,\n",
    "                name=str(group),\n",
    "                boxpoints='all',  # Show all points\n",
    "                jitter=0.5,  # Add jitter for visibility\n",
    "                whiskerwidth=0.5,\n",
    "                fillcolor=color,\n",
    "                line=dict(width=1),\n",
    "                marker=dict(size=3),\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "        # Set plot layout\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            xaxis=dict(title=grouping_column, tickvals=list(range(len(x_labels))) if x_labels else None, ticktext=x_labels),\n",
    "            yaxis=dict(title=value_column),\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "\n",
    "        # Add annotation if provided\n",
    "        if annotation:\n",
    "            fig.add_annotation(\n",
    "                text=annotation,\n",
    "                align='left',\n",
    "                showarrow=False,\n",
    "                xref='paper',\n",
    "                yref='paper',\n",
    "                x=1.05,\n",
    "                y=0.5,\n",
    "                bordercolor='black',\n",
    "                borderwidth=1,\n",
    "                bgcolor='lightgray',\n",
    "                font=dict(size=10)\n",
    "            )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in plot_boxplots_interactive: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in plot_boxplots_interactive: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in plot_boxplots_interactive: {e}\")\n",
    "        raise\n",
    "\n",
    "# -------------------- STEP 4: INTERACTIVE HISTOGRAMS -------------------- #\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_histograms_interactive(data, columns, titles, bins=30, colors=None):\n",
    "    \"\"\"\n",
    "    Create interactive histograms using Plotly for analyzing data distributions.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataset containing the columns to plot.\n",
    "    columns (list): List of column names to create histograms for.\n",
    "    titles (list): Titles for each histogram.\n",
    "    bins (int): Number of bins for histograms.\n",
    "    colors (list, optional): List of colors for the histograms.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if len(columns) != len(titles):\n",
    "            raise ValueError(\"The `columns` and `titles` lists must have the same length.\")\n",
    "        if colors and len(columns) != len(colors):\n",
    "            raise ValueError(\"If `colors` are provided, their length must match `columns` and `titles`.\")\n",
    "\n",
    "        # Loop through each column to plot\n",
    "        for idx, column in enumerate(columns):\n",
    "            if column not in data.columns:\n",
    "                raise KeyError(f\"Column '{column}' not found in the dataset.\")\n",
    "            if data[column].isnull().any():\n",
    "                logging.warning(f\"Column '{column}' contains missing values.\")\n",
    "\n",
    "            # Create the histogram\n",
    "            fig = px.histogram(\n",
    "                data_frame=data,\n",
    "                x=column,\n",
    "                nbins=bins,\n",
    "                title=titles[idx],\n",
    "                color_discrete_sequence=[colors[idx]] if colors else None,\n",
    "                marginal=\"box\",  # Add a box plot for better distribution analysis\n",
    "            )\n",
    "\n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                xaxis_title=f\"{column} (mm)\",\n",
    "                yaxis_title=\"Frequency\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "\n",
    "            # Show the plot\n",
    "            fig.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in plot_histograms_interactive: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in plot_histograms_interactive: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in plot_histograms_interactive: {e}\")\n",
    "        raise\n",
    "\n",
    "# -------------------- STEP 5: ENHANCED STATIONARITY CHECK -------------------- #\n",
    "def perform_stationarity_check(data, column, window=12):\n",
    "    \"\"\"\n",
    "    Perform enhanced stationarity checks using rolling statistics and statistical tests.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Input dataset.\n",
    "    column (str): Column to perform the stationarity check on.\n",
    "    window (int): Window size for rolling statistics.\n",
    "\n",
    "    Returns:\n",
    "    dict: Results of the statistical tests.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        if column not in data.columns:\n",
    "            raise KeyError(f\"Column '{column}' not found in the dataset.\")\n",
    "        if data[column].isnull().any():\n",
    "            logging.warning(f\"Column '{column}' contains missing values.\")\n",
    "\n",
    "        # Suppress statsmodels warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "        # Calculate rolling mean and standard deviation\n",
    "        rolling_mean = data[column].rolling(window=window).mean()\n",
    "        rolling_std = data[column].rolling(window=window).std()\n",
    "\n",
    "        # Interactive Rolling Statistics Plot\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=data['Month'], y=data[column],\n",
    "            mode='lines', name='Original Data', line=dict(color='blue')\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=data['Month'], y=rolling_mean,\n",
    "            mode='lines', name=f'Rolling Mean ({window} months)', line=dict(color='orange')\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=data['Month'], y=rolling_std,\n",
    "            mode='lines', name=f'Rolling Std Dev ({window} months)', line=dict(color='green')\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title=\"Rolling Statistics (Interactive)\",\n",
    "            xaxis_title=\"Time\",\n",
    "            yaxis_title=\"Rainfall (mm)\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # Statistical Tests\n",
    "        adf_result = adfuller(data[column].dropna())\n",
    "        try:\n",
    "            kpss_result = kpss(data[column].dropna(), regression='c', nlags=\"legacy\")\n",
    "        except ValueError as e:\n",
    "            kpss_result = (None, None)\n",
    "            logging.warning(f\"KPSS Test Error: {e}\")\n",
    "\n",
    "        pp_result = PhillipsPerron(data[column].dropna())\n",
    "        za_result = zivot_andrews(data[column].dropna())\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        results = {\n",
    "            \"ADF Test\": {\"Statistic\": adf_result[0], \"p-value\": adf_result[1]},\n",
    "            \"KPSS Test\": {\n",
    "                \"Statistic\": kpss_result[0] if kpss_result[0] is not None else \"Unavailable\",\n",
    "                \"p-value\": kpss_result[1] if kpss_result[1] is not None else \"Unavailable\",\n",
    "            },\n",
    "            \"Phillips-Perron Test\": {\"Statistic\": pp_result.stat, \"p-value\": pp_result.pvalue},\n",
    "            \"Zivot-Andrews Test\": {\"Statistic\": za_result[0], \"p-value\": za_result[1]},\n",
    "        }\n",
    "\n",
    "        # Log results\n",
    "        for test, result in results.items():\n",
    "            logging.info(f\"{test}: Statistic={result['Statistic']}, p-value={result['p-value']}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in perform_stationarity_check: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in perform_stationarity_check: {e}\")\n",
    "        raise\n",
    "\n",
    "# ------------------------------- DATA PREPARATION ------------------------------- #\n",
    "\n",
    "# ------------------------------- STEP 1: LOAD AND PREPARE DATA ------------------------------- #\n",
    "def prepare_data(file_path, scaler_path='scaler.pkl', features_to_keep=None, output_dir='output'):\n",
    "    \"\"\"\n",
    "    Load, validate, and prepare the dataset for model training.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the preprocessed dataset.\n",
    "    scaler_path (str): Path to save the MinMaxScaler object for inverse transformations.\n",
    "    features_to_keep (list, optional): List of columns to retain in the prepared dataset.\n",
    "    output_dir (str): Directory to save intermediate files and logs.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Prepared and scaled dataset.\n",
    "    MinMaxScaler: Fitted scaler object for further transformations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Validate input\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found at: {file_path}\")\n",
    "\n",
    "        logging.info(f\"Loading dataset from {file_path}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure 'Month' column is in datetime format\n",
    "        if 'Month' not in df.columns:\n",
    "            raise KeyError(\"'Month' column is missing in the dataset.\")\n",
    "        df['Month'] = pd.to_datetime(df['Month'])\n",
    "\n",
    "        # Select relevant features\n",
    "        default_features = ['Month', 'Monthly_Total', 'Rolling_Average']\n",
    "        features_to_keep = features_to_keep or default_features\n",
    "        missing_features = [col for col in features_to_keep if col not in df.columns]\n",
    "        if missing_features:\n",
    "            raise KeyError(f\"Missing required columns: {missing_features}\")\n",
    "        df_selected = df[features_to_keep]\n",
    "\n",
    "        # Validate selected features\n",
    "        numerical_columns = [col for col in features_to_keep if col != 'Month']\n",
    "        for col in numerical_columns:\n",
    "            if not np.issubdtype(df_selected[col].dtype, np.number):\n",
    "                raise ValueError(f\"Column '{col}' contains non-numeric data.\")\n",
    "            if df_selected[col].isnull().any():\n",
    "                logging.warning(f\"Column '{col}' contains missing values.\")\n",
    "            if (df_selected[col] < 0).any():\n",
    "                logging.warning(f\"Column '{col}' contains negative values.\")\n",
    "\n",
    "        # Normalize numerical features\n",
    "        logging.info(f\"Normalizing features: {numerical_columns}\")\n",
    "        scaler = MinMaxScaler()\n",
    "        df_selected[numerical_columns] = scaler.fit_transform(df_selected[numerical_columns])\n",
    "\n",
    "        # Save the scaler\n",
    "        scaler_path = os.path.join(output_dir, scaler_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logging.info(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "        # Log and return\n",
    "        logging.info(f\"Data preparation completed. Dataset contains {df_selected.shape[0]} rows and {df_selected.shape[1]} columns.\")\n",
    "        return df_selected, scaler\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"FileNotFoundError: {e}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in prepare_data: {e}\")\n",
    "        raise\n",
    "\n",
    "# ------------------------------- STEP 2: CREATE SEQUENCES ------------------------------- #\n",
    "def create_sequences(data, target_column, sequence_length, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create overlapping sequences for time-series data.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Preprocessed and scaled DataFrame containing features.\n",
    "    target_column (str): The name of the target column.\n",
    "    sequence_length (int): Number of time steps in each sequence.\n",
    "    output_dir (str, optional): Directory to save the generated sequences. If None, sequences are not saved.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Input sequences (samples, sequence_length, features).\n",
    "    np.ndarray: Target values for each sequence.\n",
    "    list: List of columns used as features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if target_column not in data.columns:\n",
    "            raise KeyError(f\"Target column '{target_column}' is not in the dataset.\")\n",
    "        if sequence_length <= 0:\n",
    "            raise ValueError(\"Sequence length must be greater than 0.\")\n",
    "        if len(data) <= sequence_length:\n",
    "            raise ValueError(\"Dataset size must be larger than the sequence length.\")\n",
    "\n",
    "        # Select feature columns\n",
    "        feature_columns = [col for col in data.columns if col not in [target_column, 'Month']]\n",
    "        if not feature_columns:\n",
    "            raise ValueError(\"No feature columns available after excluding target and 'Month' columns.\")\n",
    "\n",
    "        # Generate sequences\n",
    "        x, y = [], []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            x.append(data.iloc[i:i + sequence_length][feature_columns].values)\n",
    "            y.append(data.iloc[i + sequence_length][target_column])\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Log sequence details\n",
    "        logging.info(f\"Sequences created with shape: x={x.shape}, y={y.shape}\")\n",
    "        logging.info(f\"Feature columns used: {feature_columns}\")\n",
    "\n",
    "        # Optionally save sequences\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            np.save(os.path.join(output_dir, 'x_sequences.npy'), x)\n",
    "            np.save(os.path.join(output_dir, 'y_sequences.npy'), y)\n",
    "            logging.info(f\"Sequences saved to directory: {output_dir}\")\n",
    "\n",
    "        return x, y, feature_columns\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError in create_sequences: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"ValueError in create_sequences: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in create_sequences: {e}\")\n",
    "        raise\n",
    "\n",
    "# ------------------------------- STEP 3: TRAIN-TEST SPLIT ------------------------------- #\n",
    "def split_data(x, y, train_size=0.7, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Split the data into training, validation, and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    x (np.ndarray): Input sequences (samples, sequence_length, features).\n",
    "    y (np.ndarray): Target values for each sequence.\n",
    "    train_size (float): Fraction of data to use for training (default is 70%).\n",
    "    validation_split (float): Fraction of training data to use for validation (default is 10%).\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing train, validation, and test splits for x and y.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate split ratios\n",
    "        if not (0 < train_size < 1):\n",
    "            raise ValueError(\"`train_size` must be a float between 0 and 1.\")\n",
    "        if not (0 <= validation_split < 1):\n",
    "            raise ValueError(\"`validation_split` must be a float between 0 and 1.\")\n",
    "\n",
    "        # Determine split indices\n",
    "        train_end = int(len(x) * train_size)\n",
    "        x_train, x_test = x[:train_end], x[train_end:]\n",
    "        y_train, y_test = y[:train_end], y[train_end:]\n",
    "\n",
    "        # Further split training data for validation\n",
    "        if validation_split > 0:\n",
    "            val_end = int(len(x_train) * (1 - validation_split))\n",
    "            x_train, x_val = x_train[:val_end], x_train[val_end:]\n",
    "            y_train, y_val = y_train[:val_end], y_train[val_end:]\n",
    "        else:\n",
    "            x_val, y_val = None, None\n",
    "\n",
    "        # Log the data split sizes\n",
    "        logging.info(f\"Train data: {x_train.shape}, {y_train.shape}\")\n",
    "        if x_val is not None:\n",
    "            logging.info(f\"Validation data: {x_val.shape}, {y_val.shape}\")\n",
    "        logging.info(f\"Test data: {x_test.shape}, {y_test.shape}\")\n",
    "\n",
    "        return {\n",
    "            \"x_train\": x_train, \"y_train\": y_train,\n",
    "            \"x_val\": x_val, \"y_val\": y_val,\n",
    "            \"x_test\": x_test, \"y_test\": y_test\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data splitting: {e}\")\n",
    "        raise\n",
    "\n",
    "# Perform the split\n",
    "split_data_dict = split_data(x, y, train_size=0.7, validation_split=0.1)\n",
    "\n",
    "# Extract the splits for convenience\n",
    "x_train = split_data_dict[\"x_train\"]\n",
    "y_train = split_data_dict[\"y_train\"]\n",
    "x_val = split_data_dict[\"x_val\"]\n",
    "y_val = split_data_dict[\"y_val\"]\n",
    "x_test = split_data_dict[\"x_test\"]\n",
    "y_test = split_data_dict[\"y_test\"]\n",
    "\n",
    "# Reshape for LSTM/CNN if needed\n",
    "x_train_reshaped = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "x_test_reshaped = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
    "if x_val is not None:\n",
    "    x_val_reshaped = x_val.reshape((x_val.shape[0], x_val.shape[1], x_val.shape[2]))\n",
    "\n",
    "# Save metadata for reproducibility\n",
    "metadata = {\n",
    "    \"train_size\": len(x_train),\n",
    "    \"val_size\": len(x_val) if x_val is not None else 0,\n",
    "    \"test_size\": len(x_test),\n",
    "    \"sequence_length\": x_train.shape[1],\n",
    "    \"num_features\": x_train.shape[2]\n",
    "}\n",
    "joblib.dump(metadata, \"data_split_metadata.pkl\")\n",
    "logging.info(f\"Data split metadata saved: {metadata}\")\n",
    "\n",
    "# ------------------------------- MODEL TRAINING ------------------------------- #\n",
    "\n",
    "# ------------------------- OPTUNA-LSTM HYPERPARAMETER OPTIMIZATION ------------------------- #\n",
    "def objective_lstm(trial, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Objective function for LSTM hyperparameter optimization using Optuna.\n",
    "\n",
    "    Parameters:\n",
    "        trial (optuna.trial.Trial): Optuna trial object.\n",
    "        x_train (np.ndarray): Training input data.\n",
    "        y_train (np.ndarray): Training target data.\n",
    "        x_test (np.ndarray): Test input data.\n",
    "        y_test (np.ndarray): Test target data.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation loss for the trial.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define hyperparameter search space\n",
    "        lstm_units = trial.suggest_int(\"lstm_units\", 32, 128, step=16)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.1)\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\"])\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "        epochs = 50\n",
    "\n",
    "        # Build the model\n",
    "        model = Sequential([\n",
    "            LSTM(units=lstm_units, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])),\n",
    "            Dropout(rate=dropout_rate),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=learning_rate) if optimizer_name == \"adam\" else RMSprop(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
    "\n",
    "        # Train the model with early stopping\n",
    "        early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Return the minimum validation loss\n",
    "        return min(history.history['val_loss'])\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during trial optimization: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def optimize_lstm(x_train, y_train, x_test, y_test, n_trials=50):\n",
    "    \"\"\"\n",
    "    Optimize LSTM hyperparameters using Optuna.\n",
    "\n",
    "    Parameters:\n",
    "        x_train (np.ndarray): Training input data.\n",
    "        y_train (np.ndarray): Training target data.\n",
    "        x_test (np.ndarray): Test input data.\n",
    "        y_test (np.ndarray): Test target data.\n",
    "        n_trials (int): Number of optimization trials.\n",
    "\n",
    "    Returns:\n",
    "        dict: Best hyperparameters found by Optuna.\n",
    "        keras.Model: Trained LSTM model with best hyperparameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the study and optimize\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(lambda trial: objective_lstm(trial, x_train, y_train, x_test, y_test), n_trials=n_trials)\n",
    "\n",
    "        logging.info(f\"Best trial: {study.best_trial.number}\")\n",
    "        logging.info(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "        # Train the best model with the best parameters\n",
    "        best_params = study.best_params\n",
    "        final_model = Sequential([\n",
    "            LSTM(units=best_params[\"lstm_units\"], activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])),\n",
    "            Dropout(rate=best_params[\"dropout_rate\"]),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        final_optimizer = Adam(learning_rate=best_params[\"learning_rate\"]) if best_params[\"optimizer\"] == \"adam\" else RMSprop(learning_rate=best_params[\"learning_rate\"])\n",
    "        final_model.compile(optimizer=final_optimizer, loss=MeanSquaredError())\n",
    "\n",
    "        final_model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=50,\n",
    "            batch_size=best_params[\"batch_size\"],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = final_model.predict(x_test).flatten()\n",
    "        metrics_to_compute = [\"MSE\", \"MAE\", \"R²\", \"RMSE\", \"SMAPE\"]\n",
    "        metrics = evaluate_model(y_test, y_pred, metrics_to_compute)\n",
    "        logging.info(f\"Final model metrics: {metrics}\")\n",
    "\n",
    "        return best_params, final_model\n",
    "\n",
    "    except optuna.exceptions.OptunaError as oe:\n",
    "        logging.error(f\"Optuna encountered an error: {str(oe)}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"ValueError in optimize_lstm: {str(ve)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during LSTM optimization: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# -------------------- PREDICTION ON TRAINING DATA WITH LSTM -------------------- #\n",
    "def predict_and_visualize_training(model, x_train, y_train, scaler, feature_column='Monthly_Total', sequence_length=12):\n",
    "    \"\"\"\n",
    "    Predict on training data and visualize actual vs predicted values.\n",
    "\n",
    "    Parameters:\n",
    "        model (keras.Model): Trained LSTM model.\n",
    "        x_train (np.ndarray): Training input data.\n",
    "        y_train (np.ndarray): True target values for training data.\n",
    "        scaler (MinMaxScaler): Scaler used for normalization.\n",
    "        feature_column (str): Original feature for inverse scaling.\n",
    "        sequence_length (int): Length of input sequences.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with actual and predicted values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate predictions\n",
    "        logging.info(\"Generating predictions on training data...\")\n",
    "        y_pred = model.predict(x_train).flatten()\n",
    "\n",
    "        # Inverse transform predictions and actuals to original scale\n",
    "        y_pred_inverse = scaler.inverse_transform(\n",
    "            np.hstack([np.zeros((len(y_pred), x_train.shape[2] - 1)), y_pred.reshape(-1, 1)])\n",
    "        )[:, -1]\n",
    "\n",
    "        y_train_inverse = scaler.inverse_transform(\n",
    "            np.hstack([np.zeros((len(y_train), x_train.shape[2] - 1)), y_train.reshape(-1, 1)])\n",
    "        )[:, -1]\n",
    "\n",
    "        # Prepare DataFrame for visualization\n",
    "        training_results = pd.DataFrame({\n",
    "            'Time Index': range(sequence_length, len(y_train) + sequence_length),\n",
    "            'Actual': y_train_inverse,\n",
    "            'Predicted': y_pred_inverse\n",
    "        })\n",
    "\n",
    "        # Plot Actual vs Predicted\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.plot(training_results['Time Index'], training_results['Actual'], label='Actual', color='blue')\n",
    "        plt.plot(training_results['Time Index'], training_results['Predicted'], label='Predicted', color='orange', linestyle='--')\n",
    "        plt.title(\"LSTM Model: Actual vs Predicted on Training Data\")\n",
    "        plt.xlabel(\"Time Index\")\n",
    "        plt.ylabel(f\"Rainfall ({feature_column})\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        logging.info(\"Prediction and visualization completed successfully.\")\n",
    "\n",
    "        return training_results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in predict_and_visualize_training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# -------------------- EXECUTION -------------------- #\n",
    "try:\n",
    "    # Call the prediction function\n",
    "    training_results = predict_and_visualize_training(\n",
    "        model=lstm_model, \n",
    "        x_train=x_train, \n",
    "        y_train=y_train, \n",
    "        scaler=scaler, \n",
    "        feature_column='Monthly_Total',\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "    # Save training results for further inspection\n",
    "    training_results.to_csv(\"training_predictions.csv\", index=False)\n",
    "    logging.info(\"Training predictions saved to 'training_predictions.csv'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to predict or visualize on training data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae8bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
